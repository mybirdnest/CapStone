---
title: "Predicting Sales Win or Lose"
author: "Herman Toeante"
date: "February 12, 2019"
output:
  pdf_document: default
  html_document: default
---


## Machine Learning for predicting sales win or loss

The goal of this project is to use machine learning to predict whether or not a sales lead is either win or loss and the probability of each class. This project is a supervised classification problem.

Two models will be used to test the data; Logistic regression and Random Forest. These two models are part of the caret package or R. Before preparing the model, the first step is to choose the independent variable that will be used in the model.

```{r message = FALSE}

library(tidyverse)
library(caret)

sales_win_loss <- read_csv("DATA/WA_Fn-UseC_-Sales-Win-Loss.csv")

colnames(sales_win_loss) <- c("ID","SuppliesSubgroup","SuppliesGroup","Region", "Route",
                              "ElapsedDays", "Result","SalesStageCount",
                              "TotalDaysClosing","TotalDaysQualified",
                              "Opportunity","ClientSizeRev","ClientSizeCount",
                              "Revenue","Competitor","RDaysIdentified",
                              "RDaysValidated","RDaysQualified",
                              "DealSize")


ModelData <- sales_win_loss %>% select(SuppliesSubgroup, Region, Route, TotalDaysClosing, 
                                        TotalDaysQualified, Opportunity, ClientSizeRev, 
                                        ClientSizeCount, Competitor, DealSize, Result)
```

The next step is partitioning the data set into training, validation, and testing dataset.
We start with the seed for reproducibility followed by using createDataPartition function from caret.

```{r message = FALSE}

set.seed(3456)
TrainingValidationIndex <- caret::createDataPartition(ModelData$Result, p = .80, 
                                                      list = FALSE, 
                                                      times = 1)
str(TrainingValidationIndex)


TrainingValidation <- ModelData[ TrainingValidationIndex,]
TrainingIndex <- caret::createDataPartition(TrainingValidation$Result, p = .75, 
                                            list = FALSE, 
                                            times = 1)

Training <- TrainingValidation[TrainingIndex,]
Validation <- TrainingValidation[-TrainingIndex,]
Testing  <- ModelData[-TrainingValidationIndex,]

```

Using glimpse function, we can see the selected variables for the model.

```{r}

glimpse(Training)

```

Next, we setup the model parameter

```{r message = FALSE}

control <- caret::trainControl(method = "cv", number = 2, classProbs = TRUE)
seed <- 7
metric <- "Accuracy"
set.seed(seed)

```

Training Logistic with training dataset

```{r message = FALSE}

GLMModel <- caret::train(
  Result ~ SuppliesSubgroup + Region + Route + TotalDaysClosing + TotalDaysQualified +
    Opportunity + ClientSizeRev + ClientSizeCount + Competitor + DealSize,
  data = Training,
  method = "glm",
  trControl = control
)

```

Training Random Forest model with training dataset

```{r message = FALSE}

RFModel <- caret::train(
  Result ~ SuppliesSubgroup + Region + Route + TotalDaysClosing + TotalDaysQualified +
    Opportunity + ClientSizeRev + ClientSizeCount + Competitor + DealSize,
  data = Training,
  method = "rf",
  trControl = control
)

```

Using predict function from caret package, I will use the result to find the best model.
Caret package also have the confusion matrix function to calculate sensitivity, specificity, negative predictied value, positive predicted values, and F1 score.  The F1 score is a harmonic average of precision and recall to select the best model.

```{r}

PredGLM <- predict(GLMModel, Validation)

ConfMatGLM <- caret::confusionMatrix(
  PredGLM, factor(Validation$Result), positive = "Won", 
  mode = "everything")

ConfMatGLM

```

The F1 score for the Logistic model is `r ConfMatGLM$byClass["F1"]`
Next, we did the same thing using Random Forest model with validation data set

```{r}

PredRF <- predict(RFModel, Validation)

ConfMatRF <- caret:: confusionMatrix(PredRF, factor(Validation$Result), positive = "Won", 
                                     mode = "everything")

ConfMatRF

```

The F1 score for the Random Forest model is `r ConfMatRF$byClass["F1"]`

Because the F1 score for Random Forest model is higher than the logistic model.  The randome forest model is selected as the better model. The next step is to evaluate the Random Forest model using testing dataset.

```{r}

FinalPredRF <- predict(RFModel, Testing)

ConfMatFinalRF <- caret::confusionMatrix(FinalPredRF, factor(Testing$Result), positive = "Won", mode = "everything")

ConfMatFinalRF

```

The final prediction using testing data set (`r ConfMatFinalRF$byClass["F1"]`) appears to have about the same F1 score as using the validation data set (`r ConfMatRF$byClass["F1"]`)





